{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.debugger import Tracer # debugging\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import time\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns # prettify matplotlib\n",
    "from mpl_toolkits.mplot3d import Axes3D # for matplotlib 3D plots\n",
    "\n",
    "import numpy as np\n",
    "import sklearn.gaussian_process as gp\n",
    "import scipy.optimize\n",
    "from scipy.stats import norm # Gaussian/normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make deterministic\n",
    "np.random.seed(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda x: x * np.cos(x)\n",
    "x_min = 0\n",
    "x_max = 12\n",
    "xs = np.linspace(x_min, x_max, 100)\n",
    "ys = f(xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "plt.plot(xs, ys, 'g-')\n",
    "plt.margins(0.1, 0.1)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('cost')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inf = float('inf')\n",
    "\n",
    "def make2D(arr):\n",
    "    ''' convert a numpy array with shape (l,) into an array with shape (l,1)\n",
    "        (np.atleast_2d behaves similarly but would give shape (1,l) instead)\n",
    "    '''\n",
    "    return arr.reshape(-1, 1)\n",
    "\n",
    "def log_uniform(low, high):\n",
    "    ''' sample a random number in the interval [low, high] distributed logarithmically within that space '''\n",
    "    return np.exp(np.random.uniform(np.log(low), np.log(high)))\n",
    "\n",
    "def range_type(range_):\n",
    "    ''' determine whether the range is 'linear', 'logarithmic' or 'arbitrary'\n",
    "    range_: must be numpy array\n",
    "    note: range_ must be sorted either ascending or descending to be detected as linear or logarithmic\n",
    "    '''\n",
    "    if len(range_) == 1:\n",
    "        return 'constant'\n",
    "    # 'i' => integer, 'u' => unsigned integer, 'f' => floating point\n",
    "    elif len(range_) < 2 or range_.dtype.kind not in 'iuf':\n",
    "        return 'arbitrary'\n",
    "    else:\n",
    "        tmp = range_[1:] - range_[:-1] # differences between element i and element i+1\n",
    "        is_lin = np.all(np.isclose(tmp[0], tmp)) # same difference between each element\n",
    "        if is_lin:\n",
    "            return 'linear'\n",
    "        else:\n",
    "            tmp = np.log(range_)\n",
    "            tmp = tmp[1:] - tmp[:-1]\n",
    "            is_log = np.all(np.isclose(tmp[0], tmp))\n",
    "            if is_log:\n",
    "                return 'logarithmic'\n",
    "            else:\n",
    "                return 'arbitrary'\n",
    "        \n",
    "def sample_from(range_):\n",
    "    ''' draw a random sample from the given range in an appropriate mannor\n",
    "    ie if the range is linear: sample uniformally, if the range is logarithmic: sample log-uniformally\n",
    "    note: range_ can be in ascending or descending order\n",
    "    '''\n",
    "    assert len(range_) > 0, 'range_ must not be empty'\n",
    "    t = range_type(range_)\n",
    "    if t == 'linear':\n",
    "        return np.random.uniform(range_[0], range_[-1]) # doesn't matter if ascending order or descending\n",
    "    elif t == 'logarithmic':\n",
    "        return log_uniform(range_[0], range_[-1]) # doesn't matter if ascending order or descending\n",
    "    elif t == 'arbitrary' or t == 'constant':\n",
    "        return np.random.choice(range_)\n",
    "    else:\n",
    "        assert False, 'Invalid range type'\n",
    "        \n",
    "class Sample(object):\n",
    "    def __init__(self, config, cost):\n",
    "        self.config = config\n",
    "        self.cost = cost\n",
    "    def __repr__(self):\n",
    "        return '(config={}, cost={})'.format(config_string(self.config), self.cost)\n",
    "    def __iter__(self):\n",
    "        ''' so you can write my_config, my_cost = my_sample '''\n",
    "        yield self.config\n",
    "        yield self.cost\n",
    "    def __eq__(self, other):\n",
    "        return self.config == other.config and self.cost == other.cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.linspace(0,10, num=11)\n",
    "t = a[1:] - a[:-1] # differences between element i and element i+1\n",
    "np.all(np.isclose(t[0], t)) # same difference\n",
    "\n",
    "b = np.logspace(1,10, num=11, base=3) # base=3, showing that base doesn't matter\n",
    "t = np.log(b)\n",
    "t = t[1:] - t[:-1]\n",
    "np.all(np.isclose(t[0], t))\n",
    "\n",
    "print(range_type(np.linspace(1, 10, num=12)) == 'linear')\n",
    "print(range_type(np.linspace(10, 1, num=12)) == 'linear') # can be in descending order\n",
    "print(range_type(np.array([1,2,3])) == 'linear')\n",
    "print(range_type(np.array([0,10])) == 'linear') # can be 2 points\n",
    "print(range_type(np.logspace(1, 10, num=2)) == 'linear') # need at least 3 points to determine logarithmic\n",
    "\n",
    "print(range_type(np.logspace(1, 10, num=12)) == 'logarithmic')\n",
    "print(range_type(np.logspace(10, 1, num=12)) == 'logarithmic') # can be in descending order\n",
    "print(range_type(np.logspace(1, 10, num=12, base=14)) == 'logarithmic') # can be any base\n",
    "\n",
    "print(range_type(np.array([12, 1, 10])) == 'arbitrary')\n",
    "print(range_type(np.array([])) == 'arbitrary')\n",
    "print(range_type(np.array([\"hi\", \"there\"])) == 'arbitrary')\n",
    "\n",
    "print(range_type(np.array([\"hi\"])) == 'constant')\n",
    "print(range_type(np.array([1])) == 'constant')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Optimisation\n",
    "\n",
    "Resources\n",
    "- `https://thuijskens.github.io/2016/12/29/bayesian-optimisation/`\n",
    "- `https://github.com/thuijskens/bayesian-optimization/blob/master/python/gp.py`\n",
    "- `https://github.com/fmfn/BayesianOptimization/blob/master/examples/visualization.ipynb`\n",
    "- Lizotte, 2008\n",
    "\n",
    "The acquisition function is maximised using 'L-BFGS-B' which stands for 'Limited-Memory Broyden-Fletcher-Goldfarb-Shanno Bounded'\n",
    "https://en.wikipedia.org/wiki/Limited-memory_BFGS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acquisition Function\n",
    "Expected improvement $EI$ is one possible acquisition function:\n",
    "$$EI(\\mathbf x)=\\mathbb E\\left[max(0,\\; f(\\mathbf x)-f(\\mathbf x^+))\\right]$$\n",
    "where $f$ is the surrogate objective function and $\\mathbf x^+=$ the best known configuration so far.\n",
    "\n",
    "Maximising the expected improvement will result in the next configuration to test ($\\mathbf x$) being better ($f(\\mathbf x)$ larger) than $\\mathbf x^+$ (but note that $f$ is only an approximation to the real objective function).\n",
    "$$\\mathbf x_{\\mathrm{next}}=\\arg\\max_{\\mathbf x}EI(\\mathbf x)$$\n",
    "\n",
    "If $f$ is a Gaussian Process (which it is in this case) then $EI$ can be calculated analytically:\n",
    "\n",
    "$$EI(\\mathbf x)=\\begin{cases}\n",
    "\\left(\\mu(\\mathbf x)-f(\\mathbf x^+)\\right)\\mathbf\\Phi(Z) \\;+\\; \\sigma(\\mathbf x)\\phi(Z)  &  \\text{if } \\sigma(\\mathbf x)>0\\\\\n",
    "0 & \\text{if } \\sigma(\\mathbf x) = 0\n",
    "\\end{cases}$$\n",
    "\n",
    "$$Z=\\frac{\\mu(\\mathbf x)-f(\\mathbf x^+)}{\\sigma(\\mathbf x)}$$\n",
    "\n",
    "Where\n",
    "- $\\phi(\\cdot)=$ standard multivariate normal distribution PDF (ie $\\boldsymbol\\mu=\\mathbf 0$, $\\Sigma=I$)\n",
    "- $\\Phi(\\cdot)=$ standard multivariate normal distribution CDF\n",
    "\n",
    "a parameter $\\xi$ can be introduced to control the exploitation-exploration trade-off ($\\xi=0.01$ works well in almost all cases (Lizotte, 2008))\n",
    "\n",
    "$$EI(\\mathbf x)=\\begin{cases}\n",
    "\\left(\\mu(\\mathbf x)-f(\\mathbf x^+)-\\xi\\right)\\mathbf\\Phi(Z) \\;+\\; \\sigma(\\mathbf x)\\phi(Z)  &  \\text{if } \\sigma(\\mathbf x)>0\\\\\n",
    "0 & \\text{if } \\sigma(\\mathbf x) = 0\n",
    "\\end{cases}$$\n",
    "\n",
    "$$Z=\\begin{cases}\n",
    "\\frac{\\mu(\\mathbf x)-f(\\mathbf x^+)-\\xi}{\\sigma(\\mathbf x)}  &  \\text{if }\\sigma(\\mathbf x)>0\\\\\n",
    "0 & \\text{if }\\sigma(\\mathbf x) = 0\n",
    "\\end{cases}$$\n",
    "\n",
    "### Note\n",
    "Early on in development I was confused by the minimiser as it wasn't catching the peak of the acquisition function every time and it looked like it was instead settling randomly somewhere along the y=0 line. I no longer think this is the case. I think that the minimiser is a local optimiser and so any peak is fine, although there will usually be a single high peak, that local minimum is very thin and there are other much flatter local optima elsewhere in the parameter space. It is actually a good thing for exploring more of the parameter space that samples from other local optima are chosen occasionally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expected_improvement(xs, gp_model, best_cost, maximising_cost=False, xi=0.01):\n",
    "    ''' expected improvement acquisition function\n",
    "    xs: array of configurations to evaluate the GP at\n",
    "    gp_model: the GP fitted to the past configuration\n",
    "    best_cost: the (actual) cost of the best known configuration (either smallest or largest depending on maximising_cost)\n",
    "    maximising_cost: True => higher cost is better, False => lower cost is better\n",
    "    '''\n",
    "    mus, sigmas = gp_model.predict(xs, return_std=True)\n",
    "    sigmas = make2D(sigmas)\n",
    "    \n",
    "    sf = 1 if maximising_cost else -1   # scaling factor\n",
    "    diff = sf * (mus - best_cost - xi)  # mu(x) - f(x+) - xi\n",
    "    \n",
    "    with np.errstate(divide='ignore'):\n",
    "        Zs = diff / sigmas # produces inf where sigmas[i] == 0.0\n",
    "        \n",
    "    EIs = diff * norm.cdf(Zs)  +  sigmas * norm.pdf(Zs)\n",
    "    EIs[sigmas == 0.0] = 0.0 # replace the infs with 0s\n",
    "    \n",
    "    return EIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_samples = 3 # number of samples to obtain randomly before fitting the GP\n",
    "max_iterations = 1 # number of configurations to test\n",
    "num_restarts = 10 # number of restarts of the acquisition function optimisation\n",
    "maximising_cost = False # True => higher cost is better\n",
    "acquisition_function = expected_improvement\n",
    "prob_random = 0.1 # probability of ignoring the Bayesian optimisation and choosing randomly for the next sample\n",
    "'''\n",
    "gp_params = dict( # Gaussian process parameters\n",
    "    alpha = 1e-10,\n",
    "    kernel = gp.kernels.Matern(length_scale=1),\n",
    "    n_restarts_optimizer = 10,\n",
    "    #normalize_y = True # make the mean 0\n",
    ")\n",
    "'''\n",
    "\n",
    "gp_params = dict( # Gaussian process parameters\n",
    "    alpha = 1e-5,\n",
    "    kernel = gp.kernels.RBF(length_scale=1.0, length_scale_bounds=\"fixed\"), # the default kernel\n",
    "    #kernel = gp.kernels.Matern(length_scale=1.0, length_scale_bounds=\"fixed\"),\n",
    "    n_restarts_optimizer = 10,\n",
    "    normalize_y = True # make the mean 0 (theoretically a bad thing, see docs, but can help)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_config(config):\n",
    "    return f(config['x'])\n",
    "\n",
    "def config_to_input(config):\n",
    "    '''\n",
    "    given a config dictionary, return a numpy array containing the parameters that should be used as inputs to\n",
    "    the Bayesian optimisation algorithm, ie excluding parameters that are constant\n",
    "    '''\n",
    "    return np.array([[config['x'], config['z']]])\n",
    "\n",
    "def input_to_config(input_):\n",
    "    '''\n",
    "    given a numpy array of parameter values, return a configuration dict with all parameters present\n",
    "    (ie filling out the constant values)\n",
    "    '''\n",
    "    return {'x' : input_[0], 'z' : input_[1]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First obtain a few randomly selected samples from the objective function to begin fitting the surrogate function to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_sample():\n",
    "    return {'x' : sample_from(xs), 'z' : 1.0}\n",
    "\n",
    "# samples stored as Sample objects for storage\n",
    "samples = []\n",
    "for i in range(pre_samples):\n",
    "    config = random_sample()\n",
    "    samples.append(Sample(config, test_config(config)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: document params to GP\n",
    "gp_model = gp.GaussianProcessRegressor(**gp_params)\n",
    "\n",
    "# samples stored as numpy arrays for calculations\n",
    "sx = np.vstack([config_to_input(s.config) for s in samples])\n",
    "sy = make2D(np.array([s.cost for s in samples]))\n",
    "\n",
    "# best known configuration and the corresponding cost of that configuration\n",
    "if maximising_cost:\n",
    "    best_sample = max(samples, key=lambda s: s.cost)\n",
    "else:\n",
    "    best_sample = min(samples, key=lambda s: s.cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bayes_step_1D(xs, param_name, true_ys, gp_params, samples):\n",
    "    '''\n",
    "    xs: possible values along a single parameter\n",
    "    param_name: the name of the parameter that xs corresponds to\n",
    "    true_ys: true cost function corresponding to xs (None to omit)\n",
    "    '''\n",
    "    plt.figure(figsize=(16,8))\n",
    "    \n",
    "    plt.margins(0.1, 0.1)\n",
    "\n",
    "    plt.subplot(2, 1, 1) # nrows, ncols, plot_number\n",
    "    plt.xlabel('paramater: ' + param_name)\n",
    "    plt.ylabel('cost')\n",
    "\n",
    "    if true_ys is not None:\n",
    "        plt.plot(xs, true_ys, 'g-')\n",
    "        \n",
    "    sx = np.array([[s.config[param_name]] for s in samples])\n",
    "    sy = np.array([[s.cost] for s in samples])\n",
    "    plt.plot(sx, sy, 'bo')\n",
    "\n",
    "    gp_model = gp.GaussianProcessRegressor(**gp_params)\n",
    "    gp_model.fit(sx, sy)\n",
    "    mu, sigma = gp_model.predict(make2D(xs), return_std=True)\n",
    "    mu = mu.flatten()\n",
    "    \n",
    "    plt.plot(xs, mu, 'm-')\n",
    "    plt.fill_between(xs, mu - sigma, mu + sigma, alpha=0.3, color='y')\n",
    "    plt.plot(sample.config[param_name], sample.cost, 'y*', markersize=30, alpha=0.8)\n",
    "\n",
    "    plt.subplot(2, 1, 2) # nrows, ncols, plot_number\n",
    "    plt.xlabel('paramater: ' + param_name)\n",
    "    plt.ylabel('EI')\n",
    "    ac = acquisition_function(make2D(xs), gp_model, best_sample.cost, maximising_cost)\n",
    "    plt.plot(xs, ac, 'r-')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bayes_step_slice(xs, param_name, true_ys, gp_model, sx, sy, next_x, next_ac):\n",
    "    '''\n",
    "    xs: possible values along a single parameter\n",
    "    param_name: the name of the parameter that xs corresponds to\n",
    "    true_ys: true cost function corresponding to xs (None to omit)\n",
    "    gp_model: the gaussian process used in the Bayesian optimisation (already fitted to the data) \n",
    "    sx: sample inputs, shape = (num_samples, num_attribs)\n",
    "    sy: sample objective values, shape = (num_samples, 1)\n",
    "    next_x: config (dict) for the next sample to take\n",
    "    next_ac: (positive) acquisition function value for next_x \n",
    "    '''\n",
    "    plt.figure(figsize=(16,8))\n",
    "    plt.margins(0.1, 0.1)\n",
    "    plt.subplots_adjust(hspace=0.3)\n",
    "\n",
    "    plt.subplot(2, 1, 1) # nrows, ncols, plot_number\n",
    "    plt.xlabel('paramater: ' + param_name)\n",
    "    plt.ylabel('cost')\n",
    "    plt.title('Surrogate objective function')\n",
    "\n",
    "    if true_ys is not None:\n",
    "        plt.plot(xs, true_ys, 'g-')\n",
    "        \n",
    "    plt.plot([input_to_config(x)[param_name] for x in sx], sy, 'bo')\n",
    "\n",
    "    # take the next_x configuration and peterb the parameter `param_name` while leaving the others constant\n",
    "    def mks(x):\n",
    "        c = next_x.copy()\n",
    "        c[param_name] = x\n",
    "        return config_to_input(c)\n",
    "    inputs_ = np.vstack([mks(x) for x in xs])\n",
    "    \n",
    "    mu, sigma = gp_model.predict(inputs_, return_std=True)\n",
    "    mu = mu.flatten()\n",
    "    \n",
    "    plt.plot(xs, mu, 'm-')\n",
    "    plt.fill_between(xs, mu - sigma, mu + sigma, alpha=0.3, color='y')\n",
    "    #plt.plot(sample.config[param_name], sample.cost, 'y*', markersize=30, alpha=0.8)\n",
    "    plt.axvline(x=next_x[param_name])\n",
    "\n",
    "    plt.subplot(2, 1, 2) # nrows, ncols, plot_number\n",
    "    plt.xlabel('paramater: ' + param_name)\n",
    "    plt.ylabel('EI')\n",
    "    plt.title('acquisition function')\n",
    "    ac = acquisition_function(inputs_, gp_model, best_sample.cost, maximising_cost)\n",
    "    plt.plot(xs, ac, 'r-')\n",
    "    plt.axvline(x=next_x[param_name])\n",
    "    plt.plot(next_x[param_name], next_ac, 'b*', markersize=20, alpha=0.8)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(max_iterations):\n",
    "    gp_model.fit(sx, sy)\n",
    "    \n",
    "    # maximise the acquisition function to obtain the next configuration to test\n",
    "    # scipy has no maximise function, so instead minimise the negation of the acquisition function \n",
    "    # reshape(1,-1) => 1 sample (row) with N attributes (cols). Needed because x is passed as shape (N,)\n",
    "    neg_acquisition_function = lambda x: -acquisition_function(x.reshape(1,-1), gp_model, best_sample.cost, maximising_cost)\n",
    "    \n",
    "    # minimise the negative acquisition function\n",
    "    best_next_x = None\n",
    "    best_neg_ac = 0 # negative acquisition function value for best_next_x\n",
    "    for j in range(num_restarts):\n",
    "        starting_point = config_to_input(random_sample())\n",
    "        \n",
    "        # result is an OptimizeResult object\n",
    "        result = scipy.optimize.minimize(\n",
    "            fun=neg_acquisition_function,\n",
    "            x0=starting_point,\n",
    "            bounds=[(x_min, x_max), (0, 2)],\n",
    "            method='L-BFGS-B',\n",
    "            options=dict(maxiter=15000) # maxiter=15000 is default\n",
    "        )\n",
    "        assert result.success\n",
    "        \n",
    "        # result.fun == negative acquisition function evaluated at result.x\n",
    "        if result.fun < best_neg_ac:\n",
    "            best_next_x = result.x\n",
    "            best_neg_ac = result.fun\n",
    "    \n",
    "    # acquisition function optimisation finished:\n",
    "    # best_next_x = argmax(acquisition_function)\n",
    "    \n",
    "    if np.random.uniform() < prob_random:\n",
    "        print('choosing random sample because prob_random={}'.format(prob_random))\n",
    "        best_next_x = random_sample()\n",
    "        best_neg_ac = 1\n",
    "    elif best_next_x is None:\n",
    "        print('choosing random sample because no sample was found with acquisition value >0')\n",
    "        best_next_x = random_sample()\n",
    "        best_neg_ac = 1\n",
    "    elif np.any(np.linalg.norm(best_next_x - sx) <= 1e-7):\n",
    "        print('choosing random sample to avoid samples being too close')\n",
    "        best_next_x = random_sample() # choose randomly instead\n",
    "        best_neg_ac = 1\n",
    "    else:\n",
    "        best_next_x = input_to_config(best_next_x)\n",
    "    \n",
    "    #plot_bayes_step_1D(xs, 'x', ys, gp_params, samples)\n",
    "    plot_bayes_step_slice(xs, 'x', ys, gp_model, sx, sy, best_next_x, -best_neg_ac)\n",
    "    \n",
    "    \n",
    "    # having two samples too close together will 'break' the GP\n",
    "    # will assume that randomly chosen samples and the pre-samples are unlikely to ever be too close\n",
    "        \n",
    "    sample = Sample(best_next_x, test_config(best_next_x))\n",
    "    samples.append(sample)\n",
    "    # config and cost should be numpy arrays\n",
    "    sx = np.append(sx, config_to_input(sample.config), axis=0)\n",
    "    sy = np.append(sy, make2D(sample.cost), axis=0)\n",
    "    \n",
    "    # update current best\n",
    "    if ((maximising_cost and sample.cost > best_sample.cost) or\n",
    "       (not maximising_cost and sample.cost < best_sample.cost)):\n",
    "        best_sample = sample\n",
    "    \n",
    "    print('iteration done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "xs = np.array([[1,2], [3,4]])\n",
    "x = np.array([[3,4]])\n",
    "#np.any(np.linalg.norm(x-xs) < 1, axis=1)\n",
    "ys = xs - x\n",
    "np.any(np.linalg.norm(ys, axis=1) < 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "447px",
    "left": "1833px",
    "right": "20px",
    "top": "225px",
    "width": "511px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
